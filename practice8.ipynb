{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7e2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN 구현하여 mnist데이터셋\n",
    "#convolution의 목적은 계층적으로 인식할 수 있게 함 \n",
    "#다양한 필터를 적용하여 윤곽선,질감,털 등 각종 특징을 추출할 수 있다.\n",
    "#하지만 필터를 사람이 만들기엔 너무 비효율적이라서 신경망으로 만듬\n",
    "#CNN모델은 convolution 계층, pooling 계층, 특징들을 모아 최종 분류하는 일반적인 인공신경망 계층으로 구성됨\n",
    "#컨볼루션 게층은 이미함지의 특징을 추출하는 열할을 함\n",
    "#풀링 계층은 필터를 거친 여러 특징 중 가장 중요한 특징 하나를 고르게됨 -> 이미지 차원 감소\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd51b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필터가 움직이는 칸수를 조정해주는 것이 stride\n",
    "#stride 가 커질수록 출력되는 텐서의 크기가 작아짐\n",
    "#컨볼루션을 거쳐 만들어진 새로운 이미지는 feature map이라고 함\n",
    "#feature map이 클수록 학습이 어렵고 과적합의 위험이 증가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572dbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3823e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./.data',train = True,download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307),(0.3081))])),batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./.data',train = False,download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307),(0.3081))])),batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "#데이터셋을 불러오고 transform을 이용하여 정규화를 진행함\n",
    "#만들 CNN모델은 커널이 5*5크기이고 컨볼루션 계층은 2개임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b2c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10,20,kernel_size=5)\n",
    "        self.drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 = nn.Linear(50,10)\n",
    "\n",
    "#모델의 학습 모듈을 정의해줌\n",
    "    #다음은 입력부터 출력까지 데이터가 지나가는 길을 만들어야함\n",
    "    def forward(self,x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x),2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x),2)) #컨볼루션과 풀링을 지난 뒤 relu 활성화함수를 통과함\n",
    "    #이제 x는 컨볼루션 계층 2개를 거친 특징 맵이 됨\n",
    "    #현재 2차원인 x를 바로 인공신경망에 넣을 수 없음(인공신경망은 1차원을 받게 되어있기 때문)\n",
    "        x = x.view(-1,320) # view함수는 2차원을 1차원으로 펴주는 역할을 하고 -1은 남는차원 모두, 320은 x가 가진 원소개수를 써줌\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92449ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyunskki/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0 loss tensor(2.3477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 0 loss tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 0 loss tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 0 loss tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 0 loss tensor(0.4393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 0 loss 0.17391465904712677 acc 94.93\n",
      "train epoch 1 loss tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 1 loss tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 1 loss tensor(0.4958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 1 loss tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 1 loss tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 1 loss 0.09281325154900551 acc 96.85\n",
      "train epoch 2 loss tensor(0.2884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 2 loss tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 2 loss tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 2 loss tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 2 loss tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 2 loss 0.07443137288242578 acc 97.47\n",
      "train epoch 3 loss tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 3 loss tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 3 loss tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 3 loss tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 3 loss tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 3 loss 0.05757738885879517 acc 98.02\n",
      "train epoch 4 loss tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 4 loss tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 4 loss tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 4 loss tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 4 loss tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 4 loss 0.050294539077579976 acc 98.31\n",
      "train epoch 5 loss tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 5 loss tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 5 loss tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 5 loss tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 5 loss tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 5 loss 0.0506304746364709 acc 98.32\n",
      "train epoch 6 loss tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 6 loss tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 6 loss tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 6 loss tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 6 loss tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 6 loss 0.04731157877445221 acc 98.5\n",
      "train epoch 7 loss tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 7 loss tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 7 loss tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 7 loss tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 7 loss tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 7 loss 0.03914202969688922 acc 98.67\n",
      "train epoch 8 loss tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 8 loss tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 8 loss tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 8 loss tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 8 loss tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 8 loss 0.03994109909576364 acc 98.68\n",
      "train epoch 9 loss tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 9 loss tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 9 loss tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 9 loss tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 9 loss tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 9 loss 0.03709735824652016 acc 98.8\n",
      "train epoch 10 loss tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 10 loss tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 10 loss tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 10 loss tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 10 loss tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 10 loss 0.03724401623354061 acc 98.81\n",
      "train epoch 11 loss tensor(0.2691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 11 loss tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 11 loss tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 11 loss tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 11 loss tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 11 loss 0.03399051750227809 acc 98.96\n",
      "train epoch 12 loss tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 12 loss tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 12 loss tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 12 loss tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 12 loss tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 12 loss 0.03492463206220418 acc 99.01\n",
      "train epoch 13 loss tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 13 loss tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 13 loss tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 13 loss tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 13 loss tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 13 loss 0.03317329539349303 acc 98.93\n",
      "train epoch 14 loss tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 14 loss tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 14 loss tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 14 loss tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 14 loss tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 14 loss 0.033283944768458605 acc 98.94\n",
      "train epoch 15 loss tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 15 loss tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 15 loss tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 15 loss tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 15 loss tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 15 loss 0.03293476021719398 acc 98.93\n",
      "train epoch 16 loss tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 16 loss tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 16 loss tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 16 loss tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 16 loss tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 16 loss 0.030485936446953566 acc 98.99\n",
      "train epoch 17 loss tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 17 loss tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 17 loss tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 17 loss tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 17 loss tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 17 loss 0.029328822412970475 acc 99.02\n",
      "train epoch 18 loss tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 18 loss tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 18 loss tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 18 loss tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 18 loss tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 18 loss 0.028746916813380086 acc 99.11\n",
      "train epoch 19 loss tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 19 loss tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 19 loss tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 19 loss tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 19 loss tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 19 loss 0.02810648284088529 acc 99.05\n",
      "train epoch 20 loss tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 20 loss tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 20 loss tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 20 loss tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 20 loss tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 20 loss 0.030697220949549228 acc 99.0\n",
      "train epoch 21 loss tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 21 loss tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 21 loss tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 21 loss tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 21 loss tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 21 loss 0.026406560591328888 acc 99.05\n",
      "train epoch 22 loss tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 22 loss tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 22 loss tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 22 loss tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 22 loss tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 22 loss 0.028284955025324597 acc 99.11\n",
      "train epoch 23 loss tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 23 loss tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 23 loss tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 23 loss tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 23 loss tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 23 loss 0.026961000596918167 acc 99.15\n",
      "train epoch 24 loss tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 24 loss tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 24 loss tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 24 loss tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 24 loss tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 24 loss 0.025649929383955897 acc 99.16\n",
      "train epoch 25 loss tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 25 loss tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 25 loss tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 25 loss tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 25 loss tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 25 loss 0.02654951196955517 acc 99.13\n",
      "train epoch 26 loss tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 26 loss tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 26 loss tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 26 loss tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 26 loss tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 26 loss 0.029126907818950713 acc 99.08\n",
      "train epoch 27 loss tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 27 loss tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 27 loss tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 27 loss tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 27 loss tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 27 loss 0.027864104911498724 acc 99.17\n",
      "train epoch 28 loss tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 28 loss tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 28 loss tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 28 loss tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 28 loss tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 28 loss 0.026988542387685447 acc 99.18\n",
      "train epoch 29 loss tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 29 loss tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 29 loss tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 29 loss tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 29 loss tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 29 loss 0.024228310651192443 acc 99.21\n",
      "train epoch 30 loss tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 30 loss tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 30 loss tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 30 loss tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 30 loss tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 30 loss 0.027404754129308276 acc 99.16\n",
      "train epoch 31 loss tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 31 loss tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 31 loss tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 31 loss tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 31 loss tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 31 loss 0.026411369285709224 acc 99.11\n",
      "train epoch 32 loss tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 32 loss tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 32 loss tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 32 loss tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 32 loss tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 32 loss 0.0270945191795141 acc 99.18\n",
      "train epoch 33 loss tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 33 loss tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 33 loss tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 33 loss tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 33 loss tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 33 loss 0.028307462062919512 acc 99.16\n",
      "train epoch 34 loss tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 34 loss tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 34 loss tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 34 loss tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 34 loss tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 34 loss 0.0265914341080701 acc 99.14\n",
      "train epoch 35 loss tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 35 loss tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 35 loss tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 35 loss tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 35 loss tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 35 loss 0.025223829653684516 acc 99.21\n",
      "train epoch 36 loss tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 36 loss tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 36 loss tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 36 loss tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 36 loss tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 36 loss 0.024774219429830553 acc 99.19\n",
      "train epoch 37 loss tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 37 loss tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 37 loss tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 37 loss tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 37 loss tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 37 loss 0.028296158336452208 acc 99.18\n",
      "train epoch 38 loss tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 38 loss tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 38 loss tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 38 loss tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 38 loss tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 38 loss 0.026421580373853795 acc 99.11\n",
      "train epoch 39 loss tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 39 loss tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 39 loss tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 39 loss tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "train epoch 39 loss tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch 39 loss 0.02707125493602507 acc 99.16\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5)#최적화함수\n",
    "def train(model,train_loader,optimizer,epoch):\n",
    "    model.train()\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data,target = data.to(DEVICE),target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx%200 == 0:\n",
    "            print('train epoch',epoch,'loss',loss)\n",
    "def evaluate(model,test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target = data.to(DEVICE),target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output,target,reduction = 'sum').item()\n",
    "            pred = output.max(1,keepdim = True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100*correct/len(test_loader.dataset)\n",
    "    return test_loss,test_acc\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(model,train_loader,optimizer,epoch)\n",
    "    test_loss,test_acc = evaluate(model,test_loader)\n",
    "    print('epoch',epoch,'loss',test_loss,'acc',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa726925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
